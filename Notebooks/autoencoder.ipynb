{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "# building using the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1227 - val_loss: 0.0721\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0706 - val_loss: 0.0695\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0687 - val_loss: 0.0676\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0665 - val_loss: 0.0650\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0636 - val_loss: 0.0617\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0602 - val_loss: 0.0582\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0569 - val_loss: 0.0550\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0539 - val_loss: 0.0521\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0496\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0489 - val_loss: 0.0474\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0469 - val_loss: 0.0456\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0452 - val_loss: 0.0440\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0438 - val_loss: 0.0427\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0425 - val_loss: 0.0415\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0414 - val_loss: 0.0404\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0403 - val_loss: 0.0394\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0394 - val_loss: 0.0385\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0385 - val_loss: 0.0376\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0376 - val_loss: 0.0368\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0369 - val_loss: 0.0361\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0361 - val_loss: 0.0354\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0355 - val_loss: 0.0347\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0348 - val_loss: 0.0341\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0342 - val_loss: 0.0335\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0336 - val_loss: 0.0329\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0331 - val_loss: 0.0324\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0325 - val_loss: 0.0319\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0320 - val_loss: 0.0314\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0315 - val_loss: 0.0309\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0310 - val_loss: 0.0304\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0306 - val_loss: 0.0299\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0301 - val_loss: 0.0295\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0292 - val_loss: 0.0286\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0288 - val_loss: 0.0282\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0284 - val_loss: 0.0278\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0280 - val_loss: 0.0274\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0277 - val_loss: 0.0271\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0273 - val_loss: 0.0267\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0270 - val_loss: 0.0264\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0266 - val_loss: 0.0260\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0263 - val_loss: 0.0257\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0260 - val_loss: 0.0254\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0257 - val_loss: 0.0251\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0254 - val_loss: 0.0248\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0251 - val_loss: 0.0245\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0248 - val_loss: 0.0242\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0245 - val_loss: 0.0240\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0243 - val_loss: 0.0237\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0240 - val_loss: 0.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14ece68d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "# prepare our input data. We're using MNIST digits, and we're discarding the labels \n",
    "# since we're only interested in encoding/decoding the input images\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Reshape x_train and x_test\n",
    "x_train = np.reshape(x_train,[-1, 28*28])\n",
    "x_test = np.reshape(x_test,[-1, 28*28])\n",
    "\n",
    "# train our autoencoder for 50 epochs\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Solution:\n",
    "\n",
    "# We should create a separate encoder model\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# As well as the decoder model\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the encode images\n",
    "for i, img in enumerate(encoded_imgs[:5]):\n",
    "    encoded_img = img.reshape(8,4)\n",
    "    e_image = Image.fromarray(encoded_img, \"L\")\n",
    "    plt.imshow(encoded_img)\n",
    "    e_image.save(f\"number_{i}.png\")\n",
    "    # image.show()\n",
    "    # plot the decoded images\n",
    "    # reshaped_img = img.reshape()\n",
    "    # decoded_img = decoder.predict(x_test[i]).reshape((28, 28))\n",
    "    # recon_image = decoded_img.reshape(28, 28)\n",
    "    # decoded_img = decoder.predict(np.array([img])).reshape((28, 28))\n",
    "    decoded_img = decoded_imgs[i].reshape((28, 28))\n",
    "    # image = decoded_imgs[i]\n",
    "    # recon_image = image.reshape(28, 28)\n",
    "    d_image = Image.fromarray(decoded_img, \"L\")\n",
    "    plt.imshow(decoded_img)\n",
    "    d_image.save(f\"decoded_number{i}.png\")\n",
    "    # image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decoded images\n",
    "for img in decoded_imgs[0:5]:\n",
    "    img = img.reshape(28, 28)\n",
    "    image = Image.fromarray(img, \"L\")\n",
    "    image.save(\"number.png\")\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
