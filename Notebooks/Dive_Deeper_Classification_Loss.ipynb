{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive Deeper into Classification Loss Functions\n",
    "\n",
    "- For two-class classification problem the last layer activation function is sigmoid and loss function would be binary cross entropy\n",
    "\n",
    "- For multi-class classification problem the last layer activation function is softmax and loss function would be cross entropy\n",
    "\n",
    "- Lets learn more about binary cross entropy and cross entropy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets review sigmoid function and softmax function\n",
    "\n",
    "- Read the following blog post for 7 minutes\n",
    "\n",
    "https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Numpy\n",
    "\n",
    "- For each data sample, when we have two-class classification problem then `y_true` and `y_pred` would be scalar\n",
    "\n",
    "- For some data samples, these y_pred and y_true can also be arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred, y):\n",
    "    return (-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n"
     ]
    }
   ],
   "source": [
    "y_true = 1\n",
    "y_pred = 0.9\n",
    "# y_pred = 0.2\n",
    "\n",
    "print(binary_cross_entropy_loss(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_t = 1\n",
    "y_p = 0.9\n",
    "# y_p = 0.2\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.convert_to_tensor(y_t, dtype=tf.float32),\n",
    "\n",
    "                                               logits=tf.convert_to_tensor(np.log(y_p/(1-y_p)), dtype=tf.float32))\n",
    "binary_cross_entropy = tf.reduce_mean(cost)\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(binary_cross_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is a scalar - it should increase in proportion to how \"off\" the prediction is, no matter what direction the error is in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "0.10536054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f_k_binary_cross_entropy(y_tr, y_pr):\n",
    "    return K.mean(K.binary_crossentropy(y_tr, y_pr), axis=-1)\n",
    "\n",
    "# same input as before, it would just be an array of 1 element\n",
    "y_t = [1] \n",
    "y_p = [0.9]\n",
    "# Same output!\n",
    "print(K.eval(f_k_binary_cross_entropy(K.constant(y_t), K.constant(y_p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Entropy\n",
    "\n",
    "**What is Entropy?**\n",
    "\n",
    "In decision trees, it helped us determine information gain.\n",
    "\n",
    "**Entropy itself represents the uncertainty around a random variable.** \n",
    "For example, the entropy of flilling a fair coin is about 0.5\n",
    "However, the entropy of an unfair coin would be lower.\n",
    "\n",
    "Entropy can be both conditional and unconditional.\n",
    "\n",
    "\n",
    "What is Cross Entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/zainraza/Downloads/dev/courses/DS/DS-2.2-Deep-Learning/env/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-p[i]*np.log2(p[i]) for i in range(len(p))]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))\n",
    "print(entropy([0, 1]))  \n",
    "# note: there's no entropy in the last one! the proba of the outcomes clearly points ot a certain outcome each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "p= [1, 0]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BETTER] So we should modify our entropy implementation\n",
    "\n",
    "`np.clip` will help us handle when 0 is included in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-p[i]*np.log2(np.clip(p[i], eps, 1-eps)) for i in range(len(p))]).sum()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4426957622784505e-06\n"
     ]
    }
   ],
   "source": [
    "p= [1, 0]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "- For each data sample, when we have multi-class classification problem then `y_true` and `y_pred` would be a vector\n",
    "\n",
    "- Generally speaking, CE measures the \"similarity between 2 probability distributions\"\n",
    "\n",
    "**Is Cross Entropy a Symmetric Function?**\n",
    "\n",
    "No, not necessarily. For our implementation below for instance, p MUST equal y_t and q must equal y_p\n",
    "\n",
    "**Note**: this is **different from MSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916290731874155\n",
      "8.289306734778764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    \"\"\"y_t is the p, y_p is the q\"\"\"\n",
    "\t#return -sum([p[i]*np.log(q[i]) for i in range(len(p))])\n",
    "#     print([np.clip(q[i], eps, 1-eps) for i in range(len(p))])\n",
    "    return -sum([p[i]*np.log(np.clip(q[i], eps, 1-eps)) for i in range(len(p))])\n",
    "\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "print(cross_entropy(y_t, y_p))\n",
    "print(cross_entropy(y_p, y_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162907318741551\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def cross_entropy_via_scipy(x, y):\n",
    "        ''' SEE: https://en.wikipedia.org/wiki/Cross_entropy'''\n",
    "        return  entropy(x, y)\n",
    "    \n",
    "print(cross_entropy_via_scipy(y_t, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def keras_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8\n",
    "# The data are from the above link\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "\n",
    "print(K.eval(keras_categorical_crossentropy(K.constant(y_t), K.constant(y_p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Tensorflow\n",
    "\n",
    "Low level implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162907\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/tensorflow/tensorflow/issues/2462\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "\n",
    "y_pred_tf = tf.convert_to_tensor(y_p, np.float32)\n",
    "y_true_tf = tf.convert_to_tensor(y_t, np.float32)\n",
    "eps = 1e-6\n",
    "clipped_y_pred_tf = tf.clip_by_value(y_pred_tf, eps, 1-eps)\n",
    "loss_tf = tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(clipped_y_pred_tf)))\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(loss_tf)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another implementation of Cross Entropy in Tensorflow\n",
    "\n",
    "higher level implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91629076\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(onehot_labels=tf.convert_to_tensor(y_t, dtype=tf.float32), logits=tf.convert_to_tensor(np.log(y_p), dtype=tf.float32))\n",
    "sess = tf.Session()\n",
    "print(sess.run(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "We lots of packages to choose from! Includes:\n",
    "\n",
    "- Numpy\n",
    "- Scipy\n",
    "- Tensorflow\n",
    "- Keras\n",
    "\n",
    "### Logits()\n",
    "Logits function can also be implemented. Requires log() input\n",
    "And there's also a version that is the inverse of softmax, not just sigmoid!\n",
    "\n",
    "\n",
    "\n",
    "### Tensorflow vs. Keras\n",
    "\n",
    "Tensorflow is good for when we need to go low-level, for instance when we want to define a custom loss function (to apply in identifying image simialrity problem, for example)\n",
    "\n",
    "Typically if a problem is simple enough to use Keras - then just use Keras\n",
    "\n",
    "\n",
    "### On using different levels:\n",
    "\n",
    "Refer to Milad's Blog post to see another example of implementing a model at three different levels of abstraction in TF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How we can use Deep Learning (CNN for example) for sound classification?\n",
    "\n",
    "- What is Spectogram: https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520 \n",
    "    - this also concerns the Fourier Transform\n",
    "    - this helps us \"visualize\" sound as images\n",
    "    - relate time to energy, frequencies\n",
    "    - THEN - you can feed the images into a CNN!\n",
    "    - For example - we could build a sound classifier to relate a baby crying into different ailments (baby is hungry, needs a diaper change, etc.)\n",
    "    - HOWEVER - this is not the ONLY approach - look up how chat assisitants like Siri work as an example\n",
    "    - Milad's Observation: Spectorgram good when we have around 20 classes (so this wouldn't work in NLP for example, when you have lots more labels for the English language)\n",
    "- https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e\n",
    "\n",
    "- https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8605515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource\n",
    "\n",
    "- https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H = CE for a list of probabilities, P\n",
    "\n",
    "$H(P) = –\\sum_{x \\in X} p(x) log(p(x))$\n",
    "\n",
    "$H(P, Q) = H(P) + KL(P || Q)$\n",
    "\n",
    "$H(P, Q) != H(Q, P)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    " \n",
    "# calculate the kl divergence KL(P || Q)\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    " \n",
    "# calculate entropy H(P)\n",
    "def entropy(p):\n",
    "\treturn -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
    " \n",
    "# calculate cross entropy H(P, Q)\n",
    "def cross_entropy(p, q):\n",
    "\treturn entropy(p) + kl_divergence(p, q)\n",
    " \n",
    "# define data\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "# calculate H(P)\n",
    "en_p = entropy(p)\n",
    "print('H(P): %.3f bits' % en_p)\n",
    "# calculate kl divergence KL(P || Q)\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate cross entropy H(P, Q)\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print('H(P, Q): %.3f bits' % ce_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Review\n",
    "\n",
    "This was your last lecture of DS 2.2! Congrats!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
