{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train a CNN+MLP deep learning model with Keras with two followings for MNIST dataset:\n",
    "```\n",
    "1. Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "2. Conv2D(64, kernel_size=(3, 3), activation='relu')\n",
    "3. MaxPooling2D(pool_size=(2, 2))\n",
    "4. Dense(128, activation='relu')\n",
    "5. Dense(num_classes, activation='softmax')\n",
    "```\n",
    "Also build another model with BatchNormalization and Dropout. \n",
    "Compare these two models performance for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "# CNN and MLP architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    BatchNormalization\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "# MNIST\n",
    "from keras.datasets import mnist\n",
    "# Data normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Mathematical Computation and Timing\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28, 1)\n",
      "Shape of X_test: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Image Dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Splitting Data between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# store the number of labels \n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Reshaping Data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Displaying Resulting Dimensions\n",
    "print(f'Shape of X_train: {x_train.shape}')\n",
    "print(f'Shape of X_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Define Model (No Batch Normalization/Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instanitate a model using the Sequential API\n",
    "fully_connected = Sequential()\n",
    "\n",
    "# Convolutional Layers\n",
    "fully_connected.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "                           input_shape=(28, 28, 1)))\n",
    "fully_connected.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "fully_connected.add(MaxPooling2D(pool_size=(2, 2)))  # no learning params\n",
    "fully_connected.add(Flatten())\n",
    "\n",
    "# MLP Layers\n",
    "fully_connected.add(Dense(128, activation='relu'))\n",
    "fully_connected.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile Model\n",
    "fully_connected.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print Summary\n",
    "fully_connected.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "(No Batch Normalization or Dropout here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_fitting(fitter):\n",
    "    \"\"\"Record and display the time taken to train the model.\n",
    "    \n",
    "       Parameters:\n",
    "       fitter(function): executed to train the model\n",
    "       \n",
    "       Returns: None\n",
    "    \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    fitter()\n",
    "    end = time.time()\n",
    "    print(f'Fitting Time: {end-start} miliseconds')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Time: 179.19434523582458 miliseconds\n"
     ]
    }
   ],
   "source": [
    "time_fitting(lambda:\n",
    "                fully_connected.fit(x_train, y_train,\n",
    "                    epochs=3, batch_size=100,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model with Batch Normalization & Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,200,778\n",
      "Trainable params: 1,200,330\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define rate of dropout\n",
    "drop_rate = 0.25\n",
    "\n",
    "# Instanitate a model using the Sequential API\n",
    "partially_connected = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "partially_connected.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "                           input_shape=(28, 28, 1)))\n",
    "partially_connected.add(BatchNormalization())\n",
    "partially_connected.add(Dropout(rate=drop_rate))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "partially_connected.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "partially_connected.add(BatchNormalization())\n",
    "partially_connected.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "partially_connected.add(Dropout(rate=drop_rate))\n",
    "\n",
    "# Flatten the samples to go into MLP\n",
    "partially_connected.add(Flatten())\n",
    "\n",
    "# MLP Layer 1\n",
    "partially_connected.add(Dense(128, activation='relu'))\n",
    "partially_connected.add(BatchNormalization())\n",
    "partially_connected.add(Dropout(rate=drop_rate))\n",
    "\n",
    "# MLP Output layer\n",
    "partially_connected.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile Model\n",
    "partially_connected.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print Summary\n",
    "partially_connected.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Updated Model\n",
    "(meaning we have Batch Normalization and Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Time: 395.78104400634766 miliseconds\n"
     ]
    }
   ],
   "source": [
    "time_fitting(lambda:\n",
    "                 partially_connected.fit(x_train, y_train,\n",
    "                    epochs=3, batch_size=100,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test, signifier):\n",
    "    \"\"\"\n",
    "    Display the results of evaluating the Convolutional Neural Network.\n",
    "    \n",
    "    Parameters:\n",
    "    model(Sequential): the CNN + MLP neural network \n",
    "    x_test(np.array): the testing inputs of the MNIST dataset\n",
    "    y_test(np.array): one hot vector of testing outputs from MNIST dataset\n",
    "    signifier(str): clarifies which model is being tested: 'fully-connected'\n",
    "                    or 'partially-connected'\n",
    "    \n",
    "    Returns: None \n",
    "\n",
    "    \"\"\"\n",
    "    # compute loss and accuracy\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    # convert accuracy to percentage\n",
    "    accuracy = round(accuracy*100, 2)\n",
    "    # print the loss and accuracy\n",
    "    print(f'Loss of model with {signifier} connections: {loss}')\n",
    "    print(f'Accuracy of model with {signifier} connections: {accuracy}%')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Testing the Model without Batch Normalization or Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of model with fully-connected connections: 0.15895133051872254\n",
      "Accuracy of model with fully-connected connections: 99.0%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(fully_connected,\n",
    "               x_test, y_test,\n",
    "               'fully-connected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Testing the Model with Batch Normalization or Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of model with partially-connected connections: 1.954911483001709\n",
      "Accuracy of model with partially-connected connections: 87.76%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(partially_connected,\n",
    "               x_test, y_test,\n",
    "               'partially-connected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "\n",
    "**Which Model Performs Better?**\n",
    "\n",
    "As you can see above, the CNN model trained with full connections and no batch normalization achieved a lower loss and higher accuracy than the one that was.\n",
    "\n",
    "However even though the CNN that included dropout and batch normalization was not as accurate, it does carry some positive trade-offs. For instance, it still has an accuracy of 80%, which is respectable enough in the context of classifying hand-written digits. Additionally, this model carries less chance of being overfitted, which may make it more trustworthy to use in production. Secondly, because the model used batch normalization it was able to converge faster than the one that did not. This may suggest it uses computing resources more efficiently than the other model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
